{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "loading csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80bb84136ce77cf3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:13:48.443265431Z",
     "start_time": "2025-01-12T13:13:48.431438013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        sentence,label  \\\n",
      "0                     You guys provide EMI option?,EMI   \n",
      "1    Do you offer Zero Percent EMI payment options?...   \n",
      "2                                          0% EMI.,EMI   \n",
      "3                                              EMI,EMI   \n",
      "4                            I want in installment,EMI   \n",
      "..                                                 ...   \n",
      "323          May I please know about the offers,OFFERS   \n",
      "324                            Available offers,OFFERS   \n",
      "325                          Is offer available,OFFERS   \n",
      "326                  Want to know the discount ,OFFERS   \n",
      "327             Tell me about the latest offers,OFFERS   \n",
      "\n",
      "                                           sentence   label  \n",
      "0                      You guys provide EMI option?     EMI  \n",
      "1    Do you offer Zero Percent EMI payment options?     EMI  \n",
      "2                                           0% EMI.     EMI  \n",
      "3                                               EMI     EMI  \n",
      "4                             I want in installment     EMI  \n",
      "..                                              ...     ...  \n",
      "323              May I please know about the offers  OFFERS  \n",
      "324                                Available offers  OFFERS  \n",
      "325                              Is offer available  OFFERS  \n",
      "326                      Want to know the discount   OFFERS  \n",
      "327                 Tell me about the latest offers  OFFERS  \n",
      "\n",
      "[328 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('sofmattress_train.csv', sep='\\t')\n",
    "df[['sentence', 'label']] = df.iloc[:, 0].str.split(',', expand=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "processing text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95c6defcbe12ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        you guys provide emi option\n",
      "1      do you offer zero percent emi payment options\n",
      "2                                              0 emi\n",
      "3                                                emi\n",
      "4                              i want in installment\n",
      "                           ...                      \n",
      "323               may i please know about the offers\n",
      "324                                 available offers\n",
      "325                               is offer available\n",
      "326                        want to know the discount\n",
      "327                  tell me about the latest offers\n",
      "Name: clean_sentence, Length: 328, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "df['clean_sentence'] = df['sentence'].apply(preprocess_text)\n",
    "print(df['clean_sentence'])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:13:50.049749279Z",
     "start_time": "2025-01-12T13:13:50.045701425Z"
    }
   },
   "id": "307dc0f9f842e7f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acdc1fd05a0fd9d9"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       1.00      0.75      0.86         4\n",
      "   ABOUT_SOF_MATTRESS       0.60      1.00      0.75         3\n",
      "         CANCEL_ORDER       0.67      1.00      0.80         2\n",
      "        CHECK_PINCODE       1.00      1.00      1.00         1\n",
      "                  COD       0.50      1.00      0.67         2\n",
      "           COMPARISON       0.50      1.00      0.67         1\n",
      "    DELAY_IN_DELIVERY       0.00      0.00      0.00         2\n",
      "         DISTRIBUTORS       0.75      0.75      0.75         8\n",
      "                  EMI       1.00      0.80      0.89         5\n",
      "        ERGO_FEATURES       1.00      0.75      0.86         4\n",
      "             LEAD_GEN       1.00      0.75      0.86         4\n",
      "        MATTRESS_COST       1.00      1.00      1.00         3\n",
      "               OFFERS       1.00      0.67      0.80         3\n",
      "         ORDER_STATUS       0.50      1.00      0.67         1\n",
      "       ORTHO_FEATURES       1.00      1.00      1.00         3\n",
      "              PILLOWS       1.00      1.00      1.00         3\n",
      "     PRODUCT_VARIANTS       1.00      1.00      1.00         2\n",
      "      RETURN_EXCHANGE       0.67      0.80      0.73         5\n",
      "   SIZE_CUSTOMIZATION       1.00      1.00      1.00         1\n",
      "             WARRANTY       1.00      1.00      1.00         5\n",
      "   WHAT_SIZE_TO_ORDER       0.67      0.50      0.57         4\n",
      "\n",
      "             accuracy                           0.82        66\n",
      "            macro avg       0.80      0.85      0.80        66\n",
      "         weighted avg       0.84      0.82      0.81        66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manasa/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/manasa/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/manasa/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X = df['clean_sentence']\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectorized, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:13:52.830518486Z",
     "start_time": "2025-01-12T13:13:52.743537307Z"
    }
   },
   "id": "46e6e905984600e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345ca9f058b5c875"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for test examples:\n",
      "--------------------------------------------------\n",
      "Text: Do you have EMI options?\n",
      "Predicted Intent: EMI\n",
      "Confidence: 16.53%\n",
      "--------------------------------------------------\n",
      "Text: What are the current offers?\n",
      "Predicted Intent: OFFERS\n",
      "Confidence: 22.37%\n",
      "--------------------------------------------------\n",
      "Text: I want to know about warranty\n",
      "Predicted Intent: WARRANTY\n",
      "Confidence: 23.81%\n",
      "--------------------------------------------------\n",
      "Text: How can I cancel my order?\n",
      "Predicted Intent: CANCEL_ORDER\n",
      "Confidence: 43.80%\n",
      "--------------------------------------------------\n",
      "Text: What is the cost of mattress?\n",
      "Predicted Intent: MATTRESS_COST\n",
      "Confidence: 26.04%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to predict single input\n",
    "def predict_intent(text, model, vectorizer):\n",
    "    # Clean the input text using the same cleaning function used during training\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    \n",
    "    # Transform the text using the same vectorizer\n",
    "    text_vectorized = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = model.predict(text_vectorized)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    probabilities = model.predict_proba(text_vectorized)\n",
    "    confidence = np.max(probabilities) * 100\n",
    "    \n",
    "    return prediction[0], confidence\n",
    "\n",
    "# Test with different examples\n",
    "test_examples = [\n",
    "    \"Do you have EMI options?\",\n",
    "    \"What are the current offers?\",\n",
    "    \"I want to know about warranty\",\n",
    "    \"How can I cancel my order?\",\n",
    "    \"What is the cost of mattress?\"\n",
    "]\n",
    "\n",
    "# Print predictions\n",
    "print(\"\\nPredictions for test examples:\")\n",
    "print(\"-\" * 50)\n",
    "for text in test_examples:\n",
    "    intent, confidence = predict_intent(text, model, vectorizer)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Interactive testing\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter your text (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    intent, confidence = predict_intent(user_input, model, vectorizer)\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:14:03.343818552Z",
     "start_time": "2025-01-12T13:14:00.905624119Z"
    }
   },
   "id": "5cfbfb66e2eadb7b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMI' 'COD' 'ORTHO_FEATURES' 'ERGO_FEATURES' 'COMPARISON' 'WARRANTY'\n",
      " '100_NIGHT_TRIAL_OFFER' 'SIZE_CUSTOMIZATION' 'WHAT_SIZE_TO_ORDER'\n",
      " 'LEAD_GEN' 'CHECK_PINCODE' 'DISTRIBUTORS' 'MATTRESS_COST'\n",
      " 'PRODUCT_VARIANTS' 'ABOUT_SOF_MATTRESS' 'DELAY_IN_DELIVERY'\n",
      " 'ORDER_STATUS' 'RETURN_EXCHANGE' 'CANCEL_ORDER' 'PILLOWS' 'OFFERS']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values from column E\n",
    "unique_values = df['label'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(unique_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T10:04:57.115365854Z",
     "start_time": "2025-01-12T10:04:57.109120120Z"
    }
   },
   "id": "2c46aae908e14b0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "try creating more training data by llm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7491c94ecad82f54"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "loading file merges.txt from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "loading file tokenizer.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/manasa/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating for Monthly installment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 205\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label, content \u001B[38;5;129;01min\u001B[39;00m class_examples\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mGenerating for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 205\u001B[0m     examples \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_examples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mexamples\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m examples:\n\u001B[1;32m    208\u001B[0m         generated_data\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m    209\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m: example,\n\u001B[1;32m    210\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m: label\n\u001B[1;32m    211\u001B[0m         })\n",
      "Cell \u001B[0;32mIn[1], line 33\u001B[0m, in \u001B[0;36mgenerate_examples\u001B[0;34m(base_prompt, example_queries, num_examples)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(examples) \u001B[38;5;241m<\u001B[39m num_examples:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m         generated \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(generated\u001B[38;5;241m.\u001B[39msplit()) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m generated:\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:285\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    284\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1362\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1355\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1356\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1359\u001B[0m         )\n\u001B[1;32m   1360\u001B[0m     )\n\u001B[1;32m   1361\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1369\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1368\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1369\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1370\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1371\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1269\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1267\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1268\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1269\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:383\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    381\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 383\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/generation/utils.py:2255\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2247\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2248\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2249\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2250\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2251\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2252\u001B[0m     )\n\u001B[1;32m   2254\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2255\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2256\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2260\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2262\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2263\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2266\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2267\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2268\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2269\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2274\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2275\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3255\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   3256\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3259\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[1;32m   3260\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[1;32m   3261\u001B[0m     outputs,\n\u001B[1;32m   3262\u001B[0m     model_kwargs,\n\u001B[1;32m   3263\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   3264\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1061\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1061\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1062\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1063\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1064\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1066\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1070\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1071\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1072\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1073\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1074\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1075\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1076\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    910\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    911\u001B[0m         block\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    912\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m         output_attentions,\n\u001B[1;32m    920\u001B[0m     )\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 922\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    933\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    402\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    403\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[0;32m--> 404\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[1;32m    413\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001B[0m, in \u001B[0;36mGPT2Attention.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001B[0m\n\u001B[1;32m    331\u001B[0m     attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upcast_and_reordered_attn(\n\u001B[1;32m    332\u001B[0m         query_states, key_states, value_states, attention_mask, head_mask\n\u001B[1;32m    333\u001B[0m     )\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 335\u001B[0m     attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn_dropout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mattn_output\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    348\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_proj(attn_output)\n",
      "File \u001B[0;32m~/Desktop/Code/intent-detection-nlp/nlpvenv/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:48\u001B[0m, in \u001B[0;36msdpa_attention_forward\u001B[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_causal \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m     is_causal \u001B[38;5;241m=\u001B[39m causal_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m query\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 48\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m attn_output, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Clear GPU cache and setup logging\n",
    "torch.cuda.empty_cache()\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# Initialize generator pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',\n",
    "    device=0,\n",
    "    truncation=True,\n",
    "    max_new_tokens=50,\n",
    "    batch_size=1,\n",
    "    pad_token_id=50256  # Set padding token explicitly\n",
    ")\n",
    "def generate_examples(base_prompt, example_queries, num_examples=15):\n",
    "    prompt = f\"\"\"Task: Generate a customer service questions asked by customers to a mattresses selling company.\n",
    "Topic: {base_prompt}\n",
    "Rules: \n",
    "- Question should be less than 10 words\n",
    "- Must be related to {base_prompt}\n",
    "- Should be similar to these examples:\n",
    "{example_queries}\n",
    "\n",
    "Generate a question:\"\"\"\n",
    "    \n",
    "    examples = []\n",
    "    while len(examples) < num_examples:\n",
    "        try:\n",
    "            result = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            generated = result[0]['generated_text']\n",
    "            if len(generated.split()) <= 10 and '?' in generated:\n",
    "                examples.append(generated)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Example prompts with sample queries\n",
    "class_examples = {\n",
    "    'Monthly installment': {\n",
    "        'prompt': \" Equated Monthly Installment EMI and payment options for mattress purchase\",\n",
    "        'examples': \"\"\"\n",
    "- Do you have EMI options?\n",
    "- I want in installment\n",
    "- Down payments\"\"\"\n",
    "    },\n",
    "    'COD': {\n",
    "        'prompt': \"Cash on Delivery or COD payment for mattress\",\n",
    "        'examples': \"\"\"\n",
    "- Is COD available?\n",
    "- Do you accept cash on delivery?\n",
    "- Can I pay later on delivery \"\"\"\n",
    "    },\n",
    "    'ORTHO_FEATURES': {\n",
    "        'prompt': \"Orthopedic features and benefits of mattress\",\n",
    "        'examples': \"\"\"\n",
    "- Is it good for back pain?\n",
    "- What orthopedic features does it have?\n",
    "- Does it support spine alignment?\"\"\"\n",
    "    },\n",
    "    'ERGO_FEATURES': {\n",
    "        'prompt': \"Ergonomic features and comfort of mattress\",\n",
    "        'examples': \"\"\"\n",
    "- What ergonomic features are included?\n",
    "- How does it support body posture?\n",
    "- Tell me about comfort features\"\"\"\n",
    "    },\n",
    "    'COMPARISON': {\n",
    "        'prompt': \"Compare different mattress models\",\n",
    "        'examples': \"\"\"\n",
    "- Which available model is better?\n",
    "- Compare soft vs firm mattress\n",
    "- What's the difference between your available models?\"\"\"\n",
    "    },\n",
    "    'WARRANTY': {\n",
    "        'prompt': \"Warranty terms and coverage for mattress\",\n",
    "        'examples': \"\"\"\n",
    "- What's the warranty period?\n",
    "- What's covered in warranty?\n",
    "- How long is the warranty valid?\"\"\"\n",
    "    },\n",
    "    '100_NIGHT_TRIAL_OFFER': {\n",
    "        'prompt': \"100-night trial period for mattress\",\n",
    "        'examples': \"\"\"\n",
    "- How does trial period work?\n",
    "- Can I return during trial?\n",
    "- What's the trial period policy?\"\"\"\n",
    "    },\n",
    "    'SIZE_CUSTOMIZATION': {\n",
    "        'prompt': \"Custom size options for mattress\",\n",
    "        'examples': \"\"\"\n",
    "- Can you make custom size?\n",
    "- Is size customization possible?\n",
    "- Do you make special sizes?\"\"\"\n",
    "    },\n",
    "    'WHAT_SIZE_TO_ORDER': {\n",
    "        'prompt': \"Help choosing mattress size\",\n",
    "        'examples': \"\"\"\n",
    "- Which size should I buy?\n",
    "- What size fits my bed?\n",
    "- Recommend size for couple?\"\"\"\n",
    "    },\n",
    "    'LEAD_GEN': {\n",
    "        'prompt': \"Request for mattress information and contact\",\n",
    "        'examples': \"\"\"\n",
    "- Please contact me about details\n",
    "- Need more information\n",
    "- Request callback for details\"\"\"\n",
    "    },\n",
    "    'CHECK_PINCODE': {\n",
    "        'prompt': \"Delivery availability check by pincode address\",\n",
    "        'examples': \"\"\"\n",
    "- Do you deliver to my pincode?\n",
    "- Check delivery availability\n",
    "- Is delivery possible here?\"\"\"\n",
    "    },\n",
    "    'DISTRIBUTORS': {\n",
    "        'prompt': \"Find nearby stores and distributors\",\n",
    "        'examples': \"\"\"\n",
    "- Where is your nearest store?\n",
    "- Dealer in my city?\n",
    "- Local distributor contact?\"\"\"\n",
    "    },\n",
    "    'MATTRESS_COST': {\n",
    "        'prompt': \"Price and cost information for mattress\",\n",
    "        'examples': \"\"\"\n",
    "- What's the price?\n",
    "- How much for queen size?\n",
    "- Cost of single mattress?\"\"\"\n",
    "    },\n",
    "    'PRODUCT_VARIANTS': {\n",
    "        'prompt': \"Different types and variants of mattresses\",\n",
    "        'examples': \"\"\"\n",
    "- What types are available?\n",
    "- Different variants you have?\n",
    "- Show all mattress options\"\"\"\n",
    "    },\n",
    "    'ABOUT_SOF_MATTRESS': {\n",
    "        'prompt': \"Information about SOF mattress company\",\n",
    "        'examples': \"\"\"\n",
    "- Tell me about your company\n",
    "- Company background details?\n",
    "- About SOF mattress brand?\"\"\"\n",
    "    },\n",
    "    'DELAY_IN_DELIVERY': {\n",
    "        'prompt': \"Delayed delivery inquiries\",\n",
    "        'examples': \"\"\"\n",
    "- Why is delivery delayed?\n",
    "- When will order arrive?\n",
    "- Reason for delivery delay?\"\"\"\n",
    "    },\n",
    "    'ORDER_STATUS': {\n",
    "        'prompt': \"Check status of mattress order\",\n",
    "        'examples': \"\"\"\n",
    "- Where is my order?\n",
    "- Track order status\n",
    "- Current delivery status?\"\"\"\n",
    "    },\n",
    "    'RETURN_EXCHANGE': {\n",
    "        'prompt': \"Return and exchange policies\",\n",
    "        'examples': \"\"\"\n",
    "- How to return mattress?\n",
    "- What's the return policy?\n",
    "- Exchange possible?\"\"\"\n",
    "    },\n",
    "    'CANCEL_ORDER': {\n",
    "        'prompt': \"Cancel mattress order\",\n",
    "        'examples': \"\"\"\n",
    "- How to cancel order?\n",
    "- Cancel my booking\n",
    "- Order cancellation process?\"\"\"\n",
    "    },\n",
    "    'PILLOWS': {\n",
    "        'prompt': \"Information about pillows\",\n",
    "        'examples': \"\"\"\n",
    "- Do you sell pillows?\n",
    "- Pillow types available?\n",
    "- Price of pillows?\"\"\"\n",
    "    },\n",
    "    'OFFERS': {\n",
    "        'prompt': \"Current offers and discounts\",\n",
    "        'examples': \"\"\"\n",
    "- Any current offers?\n",
    "- What discounts available?\n",
    "- Ongoing promotional deals?\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Generate data\n",
    "generated_data = []\n",
    "for label, content in class_examples.items():\n",
    "    print(f\"\\nGenerating for {label}...\")\n",
    "    examples = generate_examples(content['prompt'], content['examples'])\n",
    "    \n",
    "    for example in examples:\n",
    "        generated_data.append({\n",
    "            'sentence': example,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(generated_data)\n",
    "df.to_csv('llm_generated_queries.csv', index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTotal generated queries: {len(df)}\")\n",
    "print(\"\\nQueries per label:\")\n",
    "print(df['label'].value_counts())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:07:04.197509462Z",
     "start_time": "2025-01-12T11:28:45.699738620Z"
    }
   },
   "id": "5c10812f9912f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression on augmented data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b148deae94db48"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            sentence   label\n",
      "0        Do you provide EMI options?     EMI\n",
      "1    What are the EMI payment terms?     EMI\n",
      "2    Is zero interest EMI available?     EMI\n",
      "3              EMI duration options?     EMI\n",
      "4       Monthly EMI payment details?     EMI\n",
      "..                               ...     ...\n",
      "310              Promotional offers?  OFFERS\n",
      "311           Any schemes available?  OFFERS\n",
      "312                   Current deals?  OFFERS\n",
      "313                Discount seasons?  OFFERS\n",
      "314            Offer period details?  OFFERS\n",
      "\n",
      "[315 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_augmented = pd.read_csv('augmented_training_data.csv')\n",
    "\n",
    "print(df_augmented)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:11:00.538763450Z",
     "start_time": "2025-01-12T13:11:00.530817625Z"
    }
   },
   "id": "1cb379a9c71b54e1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentence   label\n",
      "0                      You guys provide EMI option?     EMI\n",
      "1    Do you offer Zero Percent EMI payment options?     EMI\n",
      "2                                           0% EMI.     EMI\n",
      "3                                               EMI     EMI\n",
      "4                             I want in installment     EMI\n",
      "..                                              ...     ...\n",
      "323              May I please know about the offers  OFFERS\n",
      "324                                Available offers  OFFERS\n",
      "325                              Is offer available  OFFERS\n",
      "326                      Want to know the discount   OFFERS\n",
      "327                 Tell me about the latest offers  OFFERS\n",
      "\n",
      "[328 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_original = pd.read_csv('sofmattress_train.csv')\n",
    "\n",
    "print(df_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:12:07.559494756Z",
     "start_time": "2025-01-12T13:12:07.552720884Z"
    }
   },
   "id": "b34ef6d9b0e6e6d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combine datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e33b46119cc06e5d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           sentence   label\n",
      "0                      You guys provide EMI option?     EMI\n",
      "1    Do you offer Zero Percent EMI payment options?     EMI\n",
      "2                                           0% EMI.     EMI\n",
      "3                                               EMI     EMI\n",
      "4                             I want in installment     EMI\n",
      "..                                              ...     ...\n",
      "638                             Promotional offers?  OFFERS\n",
      "639                          Any schemes available?  OFFERS\n",
      "640                                  Current deals?  OFFERS\n",
      "641                               Discount seasons?  OFFERS\n",
      "642                           Offer period details?  OFFERS\n",
      "\n",
      "[643 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df_original, df_augmented], ignore_index=True)\n",
    "print(df_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:12:49.918911171Z",
     "start_time": "2025-01-12T13:12:49.870931884Z"
    }
   },
   "id": "789112ea37bd64d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       1.00      1.00      1.00         5\n",
      "   ABOUT_SOF_MATTRESS       0.75      0.75      0.75         4\n",
      "         CANCEL_ORDER       1.00      1.00      1.00         8\n",
      "        CHECK_PINCODE       1.00      1.00      1.00         4\n",
      "                  COD       1.00      1.00      1.00         3\n",
      "           COMPARISON       0.75      0.75      0.75         4\n",
      "    DELAY_IN_DELIVERY       0.75      1.00      0.86         3\n",
      "         DISTRIBUTORS       0.73      1.00      0.84         8\n",
      "                  EMI       1.00      1.00      1.00         6\n",
      "        ERGO_FEATURES       0.71      0.71      0.71         7\n",
      "             LEAD_GEN       0.60      0.50      0.55         6\n",
      "        MATTRESS_COST       1.00      0.90      0.95        10\n",
      "               OFFERS       1.00      1.00      1.00         9\n",
      "         ORDER_STATUS       0.88      0.78      0.82         9\n",
      "       ORTHO_FEATURES       0.83      0.71      0.77         7\n",
      "              PILLOWS       1.00      0.80      0.89         5\n",
      "     PRODUCT_VARIANTS       0.71      0.71      0.71         7\n",
      "      RETURN_EXCHANGE       1.00      0.80      0.89         5\n",
      "   SIZE_CUSTOMIZATION       0.80      0.57      0.67         7\n",
      "             WARRANTY       0.82      1.00      0.90         9\n",
      "   WHAT_SIZE_TO_ORDER       0.40      0.67      0.50         3\n",
      "\n",
      "             accuracy                           0.84       129\n",
      "            macro avg       0.84      0.84      0.84       129\n",
      "         weighted avg       0.86      0.84      0.85       129\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df_combined['clean_sentence'] = df_combined['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Prepare for modeling\n",
    "X = df_combined['clean_sentence']\n",
    "y = df_combined['label']\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectorized, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:14:09.563414383Z",
     "start_time": "2025-01-12T13:14:09.530231165Z"
    }
   },
   "id": "764598da78dd9c89"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       0.60      0.60      0.60         5\n",
      "   ABOUT_SOF_MATTRESS       0.40      0.50      0.44         4\n",
      "         CANCEL_ORDER       1.00      0.25      0.40         8\n",
      "        CHECK_PINCODE       0.25      0.25      0.25         4\n",
      "                  COD       0.50      0.33      0.40         3\n",
      "           COMPARISON       1.00      0.25      0.40         4\n",
      "    DELAY_IN_DELIVERY       0.40      0.67      0.50         3\n",
      "         DISTRIBUTORS       0.35      0.88      0.50         8\n",
      "                  EMI       0.80      0.67      0.73         6\n",
      "        ERGO_FEATURES       0.80      0.57      0.67         7\n",
      "             LEAD_GEN       0.67      0.67      0.67         6\n",
      "        MATTRESS_COST       0.46      0.60      0.52        10\n",
      "               OFFERS       0.86      0.67      0.75         9\n",
      "         ORDER_STATUS       0.50      0.44      0.47         9\n",
      "       ORTHO_FEATURES       0.67      0.57      0.62         7\n",
      "              PILLOWS       1.00      0.60      0.75         5\n",
      "     PRODUCT_VARIANTS       0.50      0.57      0.53         7\n",
      "      RETURN_EXCHANGE       0.57      0.80      0.67         5\n",
      "   SIZE_CUSTOMIZATION       1.00      0.43      0.60         7\n",
      "             WARRANTY       1.00      0.89      0.94         9\n",
      "   WHAT_SIZE_TO_ORDER       0.33      0.67      0.44         3\n",
      "\n",
      "             accuracy                           0.58       129\n",
      "            macro avg       0.65      0.57      0.56       129\n",
      "         weighted avg       0.68      0.58      0.59       129\n",
      "\n",
      "\n",
      "SVM Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       0.75      0.60      0.67         5\n",
      "   ABOUT_SOF_MATTRESS       0.30      0.75      0.43         4\n",
      "         CANCEL_ORDER       0.67      0.25      0.36         8\n",
      "        CHECK_PINCODE       0.29      0.50      0.36         4\n",
      "                  COD       0.50      0.33      0.40         3\n",
      "           COMPARISON       1.00      0.75      0.86         4\n",
      "    DELAY_IN_DELIVERY       1.00      0.67      0.80         3\n",
      "         DISTRIBUTORS       0.50      0.75      0.60         8\n",
      "                  EMI       1.00      0.67      0.80         6\n",
      "        ERGO_FEATURES       1.00      0.57      0.73         7\n",
      "             LEAD_GEN       0.50      0.67      0.57         6\n",
      "        MATTRESS_COST       0.50      0.60      0.55        10\n",
      "               OFFERS       0.86      0.67      0.75         9\n",
      "         ORDER_STATUS       0.56      0.56      0.56         9\n",
      "       ORTHO_FEATURES       0.50      0.43      0.46         7\n",
      "              PILLOWS       0.60      0.60      0.60         5\n",
      "     PRODUCT_VARIANTS       0.88      1.00      0.93         7\n",
      "      RETURN_EXCHANGE       0.67      0.80      0.73         5\n",
      "   SIZE_CUSTOMIZATION       1.00      0.43      0.60         7\n",
      "             WARRANTY       1.00      0.89      0.94         9\n",
      "   WHAT_SIZE_TO_ORDER       0.33      0.67      0.44         3\n",
      "\n",
      "             accuracy                           0.63       129\n",
      "            macro avg       0.69      0.63      0.63       129\n",
      "         weighted avg       0.70      0.63      0.64       129\n",
      "\n",
      "\n",
      "Random Forest Predictions:\n",
      "--------------------------------------------------\n",
      "Text: Do you have EMI options?\n",
      "Predicted Intent: EMI\n",
      "Confidence: 65.50%\n",
      "--------------------------------------------------\n",
      "Text: What are the current offers?\n",
      "Predicted Intent: OFFERS\n",
      "Confidence: 57.00%\n",
      "--------------------------------------------------\n",
      "Text: I want to know about warranty\n",
      "Predicted Intent: WARRANTY\n",
      "Confidence: 86.00%\n",
      "--------------------------------------------------\n",
      "Text: How can I cancel my order?\n",
      "Predicted Intent: CANCEL_ORDER\n",
      "Confidence: 61.00%\n",
      "--------------------------------------------------\n",
      "Text: What is the cost of mattress?\n",
      "Predicted Intent: MATTRESS_COST\n",
      "Confidence: 52.50%\n",
      "--------------------------------------------------\n",
      "\n",
      "SVM Predictions:\n",
      "--------------------------------------------------\n",
      "Text: Do you have EMI options?\n",
      "Predicted Intent: EMI\n",
      "Confidence: 53.39%\n",
      "--------------------------------------------------\n",
      "Text: What are the current offers?\n",
      "Predicted Intent: OFFERS\n",
      "Confidence: 49.78%\n",
      "--------------------------------------------------\n",
      "Text: I want to know about warranty\n",
      "Predicted Intent: WARRANTY\n",
      "Confidence: 67.59%\n",
      "--------------------------------------------------\n",
      "Text: How can I cancel my order?\n",
      "Predicted Intent: CANCEL_ORDER\n",
      "Confidence: 46.98%\n",
      "--------------------------------------------------\n",
      "Text: What is the cost of mattress?\n",
      "Predicted Intent: MATTRESS_COST\n",
      "Confidence: 77.14%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Load and preprocess your data\n",
    "df = pd.DataFrame(df_combined)  # Replace with your data loading method\n",
    "df['clean_sentence'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Prepare features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_sentence'])\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Random Forest Implementation\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# SVM Implementation\n",
    "svm_model = SVC(\n",
    "    kernel='linear',\n",
    "    probability=True,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    C=1.0\n",
    ")\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Prediction function for both models\n",
    "def predict_intent(text, model, vectorizer):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    text_vectorized = vectorizer.transform([cleaned_text])\n",
    "    prediction = model.predict(text_vectorized)\n",
    "    probabilities = model.predict_proba(text_vectorized)\n",
    "    confidence = np.max(probabilities) * 100\n",
    "    return prediction[0], confidence\n",
    "\n",
    "# Print classification reports\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"Do you have EMI options?\",\n",
    "    \"What are the current offers?\",\n",
    "    \"I want to know about warranty\",\n",
    "    \"How can I cancel my order?\",\n",
    "    \"What is the cost of mattress?\"\n",
    "]\n",
    "\n",
    "# Test both models\n",
    "print(\"\\nRandom Forest Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "for text in test_examples:\n",
    "    intent, confidence = predict_intent(text, rf_model, vectorizer)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nSVM Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "for text in test_examples:\n",
    "    intent, confidence = predict_intent(text, svm_model, vectorizer)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:31:03.805546514Z",
     "start_time": "2025-01-12T13:31:02.802054352Z"
    }
   },
   "id": "9dc75304c3bd01d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
